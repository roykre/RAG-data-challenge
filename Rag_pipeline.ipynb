{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hqMuzgLSNY6s"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip install haystack-ai\n",
        "pip install \"datasets>=2.6.1\"\n",
        "pip install \"sentence-transformers>=2.2.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from haystack import Document\n",
        "import os\n",
        "from getpass import getpass\n",
        "from haystack.components.generators import OpenAIGenerator\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack import Pipeline\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever"
      ],
      "metadata": {
        "id": "M9F9uvXmi8AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def load_Data()\n",
        "  dataset1 = load_dataset(\"mvbhat/verdicts\", split='train')\n",
        "  docs1 = [Document(content=doc[\"facts\"], meta={\"verdict\": doc[\"verdict\"]}) for doc in dataset1]\n",
        "  dataset2 = load_dataset(\"macadeliccc/US-SupremeCourtVerdicts\", split='train[:500]')\n",
        "  docs2 = [Document(content=doc[\"document\"], meta={\"verdict\": doc[\"summary\"]}) for doc in dataset2]\n",
        "  return docs1 + docs2\n"
      ],
      "metadata": {
        "id": "NuI_DtyIgptf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_api()\n",
        "  if \"OPENAI_API_KEY\" not in os.environ:\n",
        "      os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")\n",
        "  generator = OpenAIGenerator(model=\"gpt-3.5-turbo\")\n",
        "  return generator"
      ],
      "metadata": {
        "id": "CnEBNhAwg1tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ans_prompt()\n",
        "\n",
        "  template = \"\"\"\n",
        "    Based on the following facts and verdict only\n",
        "\n",
        "    {% for document in documents %}\n",
        "        Facts: {{ document.meta['facts'] }}\n",
        "        Verdict: {{ document.content }}\n",
        "\n",
        "    {% endfor %}\n",
        "    Please answer the question deliberately and do not add additional details:\n",
        "    Question: {{question}}\n",
        "\n",
        "    \"\"\"\n",
        "    prompt_builder = PromptBuilder(template=template)\n",
        "    return template"
      ],
      "metadata": {
        "id": "EZjGhSwthFHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bool_prompt()\n",
        "      template = \"\"\"\n",
        "      Based on the following documents, do you have enough information to provide a reliable answer to the question?\n",
        "\n",
        "      Context:\n",
        "      {% for document in documents %}\n",
        "          Facts: {{ document.content }}\n",
        "          Verdict: {{ document.meta['verdict'] }}\n",
        "      {% endfor %}\n",
        "\n",
        "      Question: {{ question }}\n",
        "      Answer: Do I have enough information to fully answer the question? Yes or No.\n",
        "      \"\"\"\n",
        "      prompt_builder = PromptBuilder(template=template)\n",
        "      return prompt_builder"
      ],
      "metadata": {
        "id": "bdp9i8UrhrRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_docstore()\n",
        "  document_store = InMemoryDocumentStore(embedding_similarity_function=\"bm25\")\n",
        "  return document_store"
      ],
      "metadata": {
        "id": "TCdjaXpJiCKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_pipeline()\n",
        "\n",
        "\n",
        "  basic_rag_pipeline = Pipeline()\n",
        "  # Add components to your pipeline\n",
        "  basic_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
        "  basic_rag_pipeline.add_component(\"retriever\", retriever)\n",
        "  basic_rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
        "  basic_rag_pipeline.add_component(\"llm\", generator)\n",
        "\n",
        "  # Connect the components to each other\n",
        "  basic_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
        "  basic_rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "  basic_rag_pipeline.connect(\"prompt_builder\", \"llm\")"
      ],
      "metadata": {
        "id": "KX6bYUPPhS3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Initalize_doc_em():\n",
        "  doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "  doc_embedder.warm_up()"
      ],
      "metadata": {
        "id": "EphuN2GwiOJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_doc_Store(doc)\n",
        "  docs_with_embeddings = doc_embedder.run(docs)\n",
        "  document_store.write_documents(docs_with_embeddings[\"documents\"])"
      ],
      "metadata": {
        "id": "q8Zit35GicL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def in_text_em()\n",
        "  text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "  text_embedder.warm_up()  # Warm up the text embedder"
      ],
      "metadata": {
        "id": "SVWpGlJLihHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def in_retriever(document_store)\n",
        "\n",
        "  retriever = InMemoryEmbeddingRetriever(document_store)\n",
        "  return retriver"
      ],
      "metadata": {
        "id": "znS81Pc9iptI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "checks pipeline"
      ],
      "metadata": {
        "id": "hp-QnT3wfsSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"write the facts u recive about Leroy Irvis\"\n",
        "\n",
        "response = basic_rag_pipeline.run({\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}})\n",
        "\n",
        "print(response[\"llm\"][\"replies\"][0])"
      ],
      "metadata": {
        "id": "SVSfa39WOSsC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main function"
      ],
      "metadata": {
        "id": "yEpCW0qggRMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def generate_answer(query, initial_top_k=5, increment_step=5, max_attempts=3):\n",
        "    \"\"\"\n",
        "    Retrieve documents and print similarity scores between the query and each document.\n",
        "\n",
        "    Parameters:\n",
        "    - query (str): The input query for retrieval.\n",
        "    - initial_top_k (int): The initial number of documents to retrieve.\n",
        "    - increment_step (int): The number of additional documents to retrieve if needed.\n",
        "    - max_attempts (int): The maximum number of attempts to retrieve more documents.\n",
        "    \"\"\"\n",
        "    top_k = initial_top_k\n",
        "    attempt = 0\n",
        "    is_enough_info = False\n",
        "\n",
        "    while not is_enough_info and attempt < max_attempts:\n",
        "        attempt += 1\n",
        "\n",
        "        # Embed the query\n",
        "        query_embedding = text_embedder.run(query)[\"embedding\"]\n",
        "\n",
        "        # Retrieve top_k relevant documents\n",
        "        retrieval_results = retriever.run(query_embedding=query_embedding, top_k=top_k)\n",
        "        retrieved_docs = retrieval_results[\"documents\"]\n",
        "\n",
        "\n",
        "\n",
        "        # Create an assessment prompt\n",
        "        assessment_prompt_template = \"\"\"\n",
        "        Based on the following documents, do you have enough information to provide a reliable answer to the question?\n",
        "\n",
        "        Context:\n",
        "        {% for document in documents %}\n",
        "            Facts: {{ document.content }}\n",
        "            Verdict: {{ document.meta['verdict'] }}\n",
        "        {% endfor %}\n",
        "\n",
        "        Question: {{ question }}\n",
        "        Answer: Do I have enough information to fully answer the question? Yes or No.\n",
        "        \"\"\"\n",
        "        assessment_prompt_builder = PromptBuilder(template=assessment_prompt_template)\n",
        "        assessment_prompt = assessment_prompt_builder.run(documents=retrieved_docs, question=query)[\"prompt\"]\n",
        "\n",
        "        # Ask the LLM if there is enough information\n",
        "        assessment_result = generator.run(prompt=assessment_prompt)\n",
        "        assessment_reply = assessment_result.get(\"replies\", [])[0] if assessment_result.get(\"replies\") else \"\"\n",
        "\n",
        "        print(\"Assessment Result:\")\n",
        "        print(assessment_reply)\n",
        "\n",
        "        # Check the LLM's response\n",
        "        if \"Yes\" in assessment_reply:\n",
        "            is_enough_info = True\n",
        "        else:\n",
        "            print(f\"Not enough information, attempting to retrieve more documents (Attempt {attempt})...\")\n",
        "            top_k += increment_step\n",
        "\n",
        "    # Build the prompt with the retrieved documents\n",
        "    if is_enough_info:\n",
        "        temperature = 0.2\n",
        "\n",
        "      # Run the prompt builder with the retrieved documents and the query\n",
        "        final_prompt = prompt_builder.run(\n",
        "          documents=retrieved_docs,\n",
        "          question=query,\n",
        "          params={\"temperature\": temperature}  # Pass temperature here if supported\n",
        "        )[\"prompt\"]\n",
        "\n",
        "        # Generate the answer using the LLM\n",
        "        result = generator.run(prompt=final_prompt)\n",
        "\n",
        "        # Extract and print only the replies\n",
        "        replies = result.get(\"replies\", [])\n",
        "        if replies:\n",
        "            for reply in replies:\n",
        "                print(\"Generated Answer:\")\n",
        "                print(reply)\n",
        "        else:\n",
        "            print(\"No replies found\")\n",
        "    else:\n",
        "        print(\"Unable to retrieve enough information after multiple attempts.\")\n",
        "\n",
        "# Example usage\n",
        "query = \"If I stole but expressed remorse, is there a difference in punishment because I expressed remorse? What was the punishment if I didn't express remorse?\"\n",
        "generate_answer(query, initial_top_k=5, increment_step=2, max_attempts=5)\n"
      ],
      "metadata": {
        "id": "YNHjpjGmx8dS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check similarity between query and doc"
      ],
      "metadata": {
        "id": "cO9MLGdkf8xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def generate_answer(query, top_k=1):\n",
        "    \"\"\"\n",
        "    Retrieve documents and print similarity scores between the query and each document's facts.\n",
        "\n",
        "    Parameters:\n",
        "    - query (str): The input query for retrieval.\n",
        "    - top_k (int): The maximum number of documents to retrieve.\n",
        "    \"\"\"\n",
        "    # Embed the query\n",
        "    query_embedding_result = text_embedder.run(query)\n",
        "    # Ensure the query embedding is in the correct format\n",
        "    query_embedding = np.array(query_embedding_result[\"embedding\"])\n",
        "    if query_embedding.ndim > 1:\n",
        "        query_embedding = query_embedding.flatten()\n",
        "\n",
        "    # Retrieve top_k relevant documents\n",
        "    retrieval_results = retriever.run(query_embedding=query_embedding, top_k=top_k)\n",
        "    retrieved_docs = retrieval_results[\"documents\"]\n",
        "\n",
        "    # Calculate and print the similarity measure between the query and each document's facts\n",
        "    print(\"Similarity Scores for Retrieved Documents:\")\n",
        "    for i, doc in enumerate(retrieved_docs, 1):\n",
        "        # Ensure document embeddings are properly formatted\n",
        "        if 'embedding' in doc.meta and doc.meta['embedding'] is not None:\n",
        "            document_embedding = np.array(doc.meta['embedding'])\n",
        "            if document_embedding.ndim > 1:\n",
        "                document_embedding = document_embedding.flatten()\n",
        "        else:\n",
        "            doc_embedding_result = text_embedder.run(doc.content)\n",
        "            document_embedding = np.array(doc_embedding_result[\"embedding\"])\n",
        "            if document_embedding.ndim > 1:\n",
        "                document_embedding = document_embedding.flatten()\n",
        "\n",
        "        query_embedding_reshaped = query_embedding.reshape(1, -1)\n",
        "        document_embedding_reshaped = document_embedding.reshape(1, -1)\n",
        "        similarity_score = cosine_similarity(query_embedding_reshaped, document_embedding_reshaped)[0][0]\n",
        "\n",
        "        print(f\"Document {i}:\")\n",
        "        print(f\"Similarity Score: {similarity_score:.2f}\")\n",
        "       # print(f\"Facts: {doc.content}\")\n",
        "       # print(f\"Verdict: {doc.meta.get('verdict', 'No Verdict Available')}\\n\")\n",
        "\n",
        "# Example usage\n",
        "query = \"What is the minimum punishment for eating ice cream?\"\n",
        "generate_answer(query, top_k=30)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "noXaxgqUYrI0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "most generic way to Initializ the program"
      ],
      "metadata": {
        "id": "r4-voXd-gbHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Initializing the DocumentStore\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "document_store = InMemoryDocumentStore(embedding_similarity_function=\"bm25\")\n",
        "#----------------------------------------\n",
        "#fetch the data\n",
        "from datasets import load_dataset\n",
        "from haystack import Document\n",
        "\n",
        "dataset1 = load_dataset(\"mvbhat/verdicts\", split='train')\n",
        "docs1 = [Document(content=doc[\"facts\"], meta={\"verdict\": doc[\"verdict\"]}) for doc in dataset1]\n",
        "dataset2 = load_dataset(\"macadeliccc/US-SupremeCourtVerdicts\", split='train[:500]')\n",
        "docs2 = [Document(content=doc[\"document\"], meta={\"verdict\": doc[\"summary\"]}) for doc in dataset2]\n",
        "docs = docs1 + docs2\n",
        "\n",
        "\n",
        "#----------------------------------------\n",
        "#Initalize a Document Embedder\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "\n",
        "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "doc_embedder.warm_up()\n",
        "#---------------------------------------\n",
        "#Write Documents to the DocumentStore\n",
        "docs_with_embeddings = doc_embedder.run(docs)\n",
        "document_store.write_documents(docs_with_embeddings[\"documents\"])\n",
        "\n",
        "#---------------------------------------\n",
        "#Initialize a Text Embedder\n",
        "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
        "\n",
        "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "text_embedder.warm_up()  # Warm up the text embedder\n",
        "#---------------------------------------\n",
        "#Initialize the Retriever\n",
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "\n",
        "retriever = InMemoryEmbeddingRetriever(document_store)\n",
        "#--------------------------------------\n",
        "#Define a Template Prompt\n",
        "from haystack.components.builders import PromptBuilder\n",
        "template = \"\"\"\n",
        "  Based on the following facts and verdict only\n",
        "\n",
        "  {% for document in documents %}\n",
        "      Facts: {{ document.meta['facts'] }}\n",
        "      Verdict: {{ document.content }}\n",
        "\n",
        "  {% endfor %}\n",
        "  Please answer the question deliberately and do not add additional details:\n",
        "  Question: {{question}}\n",
        "\n",
        "  \"\"\"\n",
        "prompt_builder = PromptBuilder(template=template)\n",
        "#-------------------------------------\n",
        "#Initialize a Generator\n",
        "import os\n",
        "from getpass import getpass\n",
        "from haystack.components.generators import OpenAIGenerator\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")\n",
        "generator = OpenAIGenerator(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "#--------------------------------\n",
        "# build the pipeline\n",
        "from haystack import Pipeline\n",
        "\n",
        "basic_rag_pipeline = Pipeline()\n",
        "# Add components to your pipeline\n",
        "basic_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
        "basic_rag_pipeline.add_component(\"retriever\", retriever)\n",
        "basic_rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
        "basic_rag_pipeline.add_component(\"llm\", generator)\n",
        "\n",
        "# Connect the components to each other\n",
        "basic_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
        "basic_rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "basic_rag_pipeline.connect(\"prompt_builder\", \"llm\")"
      ],
      "metadata": {
        "id": "4amBZe5LgZO3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
